# An√°lisis del FC Barcelona temporada 2008-2009

## üìå 1. Objetivo del proyecto

Este proyecto tiene como objetivo construir un pipeline de datos tipo ETL (Extracci√≥n, Transformaci√≥n y Carga) de forma local, utilizando un dataset p√∫blico relacionado con f√∫tbol. A trav√©s de este ejercicio se busca aprender e implementar conceptos clave de ingenier√≠a de datos como conexi√≥n a bases de datos, manejo de logs y excepciones, control de calidad, pruebas de unidad y exportaci√≥n de resultados. El an√°lisis se centra en preparar la informaci√≥n de los partidos del FC Barcelona durante la temporada 2008-2009 como caso pr√°ctico.

## üìÅ 2. Dataset utilizado

El dataset utilizado fue **"Football Data from Transfermarkt"**, que fue obtenido desde la plataforma Kaggle. Este DataSet contiene informaci√≥n historica de ligas europeas con datos como: equipos, partidos, ligas y atributos tecnicos.


## üîç 3. Alcance y casos de uso

El alcance de este proyecto es contruir un pipeline ETL (Extracci√≥n, Transformaci√≥n y Carga) utilizando python de forma local, a partir de una base de datos SQLite obtenida de Kaggle sobre futbol europeo, el objetivo del proyecto es aprender y aplicar buenas practicas en la ingenieria de datos, incluyendo:

-Conexi√≥n a una base de datos relacional
-Extracci√≥n de datos relevantes mediante consultas SQL.
-Transformaci√≥n de datos para filtrar partidos del FC Barcelona temporada 2008-2009.
-Cargar los datos transformados en .CSV
-Registro de logs, manejo de excepciones, validaciones de calidad y pruebas unitarias.

El proyecto **no busca construir dashboards o hacer an√°lisis estad√≠stico profundo**, sino **enfocarse exclusivamente en el dise√±o y ejecuci√≥n del proceso ETL de manera reproducible y organizada**.

## üîç 4. An√°lisis exploratorio de datos (EDA)

Durante el an√°lisis exploratorio se realizaron los siguientes pasos:

- Se identificaron las tablas disponibles en la base de datos.
- Se contaron los registros por tabla.
- Se inspeccionaron las columnas y sus tipos de datos.
- Se validaron las fechas m√≠nimas y m√°ximas para entender el rango temporal.
- Se identificaron valores nulos en columnas clave.
- Se depuraron columnas irrelevantes y se seleccionaron solo las necesarias para el an√°lisis del FC Barcelona.

Se decidi√≥ no tratar los valores nulos ya que representaban un porcentaje muy bajo del total (<2%).

## üß± 5. Modelo de datos y arquitectura

A. Modelo de datos conceptual (Modelo estrella)

Para estructurar el analisis, se propuso un modelo estrella, donde la tabla de hechos es Match, y la tabla de dimensiones son:

- Team: Informaci√≥n del equipo.
- Team_attributes: Caracteristicas t√°cticas del equipo.
- League: Liga en que se jug√≥ el partido.
- Player_attributes: Atributos de los jugadores.
- Player: Informaci√≥n base de los jugadores.

Este modelo facilita el an√°lisis de desempe√±o del FC Barcelona en la temporada 2008-2009, relacionando partidos con jugadores, equipos y ligas.

B. Arquitectura del pipeline ETL

El pipeline se desarroll√≥ de forma local, a continuaci√≥n se describe el proceso del ETL:

    1. Extracci√≥n: Se hace una consulta SQL desde python a la base SQL Lite usando sqlite3 y se transforma en un dataframe.

    2. Transformaci√≥n: Se filtran los partidos del FC Barcelona de la temporada 2008 - 2009, tambi√©n se eliminan los valores nullos.

    3. Carga: Los datos finales se guardan en formato .CSV en la carpeta data/."

Este pipeline esta automatizado con un script etl.py y controlado con loggin. Adem√°s, se realizaron pruebas unitarias para verificar que la extracci√≥n y transformaci√≥n funcionen correctamente. 

## üßº 6. Pipeline ETL

Proceso ETL (Extract, Transform, Load)

Este proyecto implementa un proceso de ETL b√°sico, basado en Python para analizar los partidos del FC Barcelona en la temporada 2008 - 2009.

    üîπ Extracci√≥n (Extract)

        Se realiza una conexi√≥n local a la base de datos database.sqlite usando sqlite3.

        Se ejecuta una consulta SQL que une las tablas Match y Team para traer informaci√≥n como:

        match_api_id, date, home_team_goal, away_team_goal

        Nombre del equipo local (home_team_name) y visitante (away_team_name)

        La consulta se ejecuta desde una funci√≥n en Python (extract_match_data) con manejo de excepciones y logs.

    üîπ Transformaci√≥n (Transform)
        Se convierte la columna date al formato datetime.

        Se filtran los partidos en los que particip√≥ el FC Barcelona, ya sea como local o visitante.

        Se seleccionan solo los partidos disputados entre julio de 2008 y junio de 2009.

        Se eliminan registros nulos.

        Se incorporan controles de calidad (validaci√≥n de resultados vac√≠os, logging de resultados).

    üîπ Carga (Load)

        El DataFrame resultante se guarda como un archivo .csv en la carpeta /data/, con el nombre:

## ‚úÖ 7. Control de calidad y pruebas

Control de Calidad y Pruebas

Control de Calidad en la Transformaci√≥n
    
    Durante el proceso de transformaci√≥n, se implementaron validaciones b√°sicas para garantizar que los datos cumplieran con ciertos requisitos m√≠nimos. Por ejemplo:

        Conversi√≥n del campo date a formato datetime.

        Filtro para incluir solo partidos donde el FC Barcelona particip√≥ como local o visitante.

        Validaci√≥n para asegurar que el DataFrame resultante no est√© vac√≠o.

        Eliminaci√≥n de valores nulos (dropna), ya que representaban un porcentaje peque√±o del total.

Se agreg√≥ un sistema de logging, que guarda en la carpeta logs/ los eventos importantes del proceso: conexiones exitosas, cantidad de registros extra√≠dos, errores, advertencias, etc.

Pruebas Unitarias

    Se implementaron dos pruebas unitarias en el archivo test_etl.py:

        1. test_extract_returns_dataframe: verifica que la funci√≥n de extracci√≥n retorne un DataFrame.

        2. test_transform_filters_data: asegura que la transformaci√≥n filtre correctamente partidos del FC Barcelona y que el DataFrame no est√© vac√≠o.

## üìñ 8. Diccionario de datos

Columna | Tipo de dato | Descripci√≥n

match_api_id | Integer | Identificador unico de partido
date | Datetime | Fecha en la que se jug√≥ el partido
home_team_goal | Integer | Goles anotados por el equipo local
away_team_goal | Integer | Goles anotados por el equipo visitante
home_team_name | String	| Nombre del equipo local
away_team_name | String	| Nombre del equipo visitante

## üîÅ 9. Frecuencia de actualizaci√≥n propuesta

Frecuencia sugerida:

Mensual y trimestral

Justificaci√≥n:

    - El an√°lisis realizado corresponde a una temporada hist√≥rica de f√∫tbol (2008-2009), lo que implica que los datos no cambian en el tiempo.
    -Por lo tanto, no es necesario actualizarlos constantemente, ya que no hay nuevas entradas ni cambios.
    -Sin embargo, si el proyecto se ampliara a incluir temporadas m√°s recientes o futuras, podr√≠a considerarse una frecuencia de:
        -Mensual: si se recibe informaci√≥n frecuente (por ejemplo, datos de partidos al cierre de cada mes).
        Semanal: si se quisiera mantener un seguimiento actualizado de una liga en tiempo real.

Conclusi√≥n:

Para el caso actual (an√°lisis de una temporada hist√≥rica), no se requiere una actualizaci√≥n peri√≥dica. Si se extendiera el proyecto a nuevas temporadas, se podr√≠a establecer una frecuencia de actualizaci√≥n mensual.

## üìÇ 12. Estructura del proyecto


‚îú‚îÄ‚îÄ database.sqlite                  # Base de datos original descargada desde Kaggle
‚îú‚îÄ‚îÄ etl.py                           # Script principal del pipeline ETL (Extract, Transform, Load)
‚îú‚îÄ‚îÄ test_etl.py                      # Archivo de pruebas unitarias para validar el ETL
‚îú‚îÄ‚îÄ data/                            # Carpeta de salida donde se guarda el CSV final transformado
‚îÇ   ‚îî‚îÄ‚îÄ barcelona_2008_2009.csv
‚îú‚îÄ‚îÄ logs/                            # Carpeta con los logs de ejecuci√≥n
‚îÇ   ‚îî‚îÄ‚îÄ proyecto.log
‚îú‚îÄ‚îÄ README.md                        # Documentaci√≥n principal del proyecto
‚îú‚îÄ‚îÄ Documentaci√≥n prueba t√©cnica.docx  # Archivo Word con notas y avances del proyecto
‚îú‚îÄ‚îÄ Modelo_Estrella.drawio          # Diagrama conceptual del modelo de datos
‚îî‚îÄ‚îÄ Proyecto_futbol.ipynb           # Exploraci√≥n inicial del dataset y EDA (an√°lisis exploratorio)

## 13. Criterio de reproducibilidad:

Para ejecutar el proyecto y reproducir los resultados, se deben seguir los siguientes pasos:

1. Clonar el repositorio:

git clone ***

2. Instalar los requerimientos (si aplicas entorno virtual):

pip install pandas

3. Verificar que el archivo database.sqlite est√© en la ra√≠z del proyecto.

4. Ejecutar el pipeline ETL:

python etl.py

5. Ejecutar las pruebas unitarias:

python test_etl.py

6. El archivo resultante estar√° en:

data/barcelona_2008_2009.csv

## 14. Escenarios alternos:

üìà Escalabilidad (datos se incrementan 100x)

En caso de que el volumen de datos aumente significativamente:

    1. Se recomienda migrar de SQLite a un motor m√°s robusto como PostgreSQL o Amazon Redshift.
    2. Usar procesamiento distribuido con herramientas como Apache Spark para manejar grandes vol√∫menes.
    3. Almacenar los archivos en un sistema escalable como Amazon S3.

‚è∞ Ejecuci√≥n diaria en ventana de tiempo espec√≠fica

Si las tuber√≠as deben ejecutarse autom√°ticamente todos los d√≠as:

    1. Utilizar Apache Airflow, AWS Step Functions o cron jobs para orquestar las tareas ETL.
    2. Validar los logs y m√©tricas de ejecuci√≥n autom√°ticamente.
    3. En ambientes cloud, habilitar trigger por eventos o CloudWatch Events (AWS).

üë• Acceso concurrente por m√°s de 100 usuarios

    1. Usar un motor de base de datos transaccional como PostgreSQL, BigQuery o Snowflake, que soportan alta concurrencia.
    2. Implementar caching con Redis o Memcached para evitar sobrecargar la base.
    3. Separar entornos: uno para ingesta/ETL y otro para consultas de usuarios.

‚ö°Ô∏è Analitica en tiempo real

Si se requiere analitica en tiempo real

    1. Cambiar el modelo batch por uno streaming, usando herramientas como:
        a. Kafka para ingesta
        b. Apache Flink o Spark Streaming para procesamiento
        c. DynamoDB, ClickHouse o Elasticsearch para consultas r√°pidas.
    2. Visualizar en tiempo real con herramientas como Grafana o Power BI Streaming Datasets.
